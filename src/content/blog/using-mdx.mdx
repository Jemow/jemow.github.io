---
title: 'OpenGL ES Renderer'
description: 'A technical devlog of my 2nd-year project: building a real-time 3D scene using OpenGL ES.'
pubDate: 2026-02-03
updatedDate: 2026-02-04
heroImage: ../../assets/cars1_tuner_scene.png
author: Jemo
tags:
  - Engine
  - Graphics
  - OpenGL
---

# Introduction

As part of my 2nd-year project at SAE Institute Geneva,
I was tasked with building a 3D scene using C++ and OpenGL ES 3.0.

This project was a great opportunity to explore the low-level workings of the GPU.
In this blog post, I will present the rendering techniques I used, focusing
on key concepts rather than specific implementation details.
For a deeper dive, I highly recommend reading [LearnOpenGL](https://learnopengl.com/), which
served as my main reference throughout this module.

# Engine

## Model

It all starts with importing geometry. I opted for standard Wavefront (.obj) models for their simplicity.
For the scene, I chose iconic models from the movie *Cars* (notably Wingo).
However, to achieve a realistic look in my pipeline, I generated compatible textures using **PBR Forge** since 
the original models only came with basic textures.

![Wingo](/assets/wingo.png)

To import the 3D models, I used the [Assimp](https://github.com/assimp/assimp) library, which handles generating vertices,
indices, and materials.

When loading, I use specific flags to process the data: `aiProcess_Triangulate` to ensure everything is converted 
to triangles, and `aiProcess_CalcTangentSpace` to calculate the tangents needed for Normal Mapping.
au Normal Mapping.
```cpp
Assimp::Importer importer;

const aiScene* scene = importer.ReadFile(
    path,
    aiProcess_Triangulate |
    aiProcess_FlipUVs |
    aiProcess_CalcTangentSpace
);
```
One important detail: these models aren't just a single block.
They are made up of several distinct **Sub-Meshes**.
That‚Äôs why I use a recursive function, `ProcessNode`, to traverse the entire 
model hierarchy and extract and store each part of the car independently.
```cpp
void Model::ProcessNode(const aiNode* node, const aiScene* scene)
{
    for (unsigned int i = 0; i < node->mNumMeshes; i++)
    {
        const aiMesh* mesh = scene->mMeshes[node->mMeshes[i]];
        sub_meshes_.push_back(ProcessMesh(mesh, scene));
    }

    for (unsigned int i = 0; i < node->mNumChildren; i++)
        ProcessNode(node->mChildren[i], scene);
}
````
The `ProcessMesh` method takes care of converting Assimp's data into my own data structures. This is where I grab the positions,
normals, and UVs to create my vertices.

## Meshes & Vertex Attributes

Once the mesh data is loaded, it needs to be sent to the GPU.
To ensure the shader can correctly interpret the raw data, I use a `VertexBufferAttribute` 
abstraction that contains all the necessary information to describe a vertex.
```cpp
struct VertexBufferAttribute {
  GLuint location;
  GLint size;
  GLenum type;
  GLsizei stride;
  size_t offset;
};
```
This tells OpenGL exactly how my vertex is structured: where to find the position, normal,
and UVs, as well as the tangents and bitangents essential for Normal Mapping. 
I use `offsetof` to automatically calculate the memory offsets.
```cpp
constexpr common::VertexBufferAttribute attributes[] = {
    { 0, 3, GL_FLOAT, sizeof(Vertex), offsetof(Vertex, Position) },
    { 1, 3, GL_FLOAT, sizeof(Vertex), offsetof(Vertex, Normal) },
    { 2, 2, GL_FLOAT, sizeof(Vertex), offsetof(Vertex, TexCoords) },
    { 3, 3, GL_FLOAT, sizeof(Vertex), offsetof(Vertex, Tangent) },
    { 4, 3, GL_FLOAT, sizeof(Vertex), offsetof(Vertex, Bitangent) }
};
vertex_input_.BindVertexBuffer(vertex_buffer_, attributes);
```

## Materials & Textures

![Textures](/assets/textures.png)
To load the textures associated with the model, I created a `LoadMaterial` method.
It uses a handy little lambda to check if a texture exists in the .obj file (via Assimp)
and automatically loads it into the correct slot of my Material structure.

```cpp
Material Model::LoadMaterial(const aiMaterial* mat) const
{
    Material m;

    auto LoadTex = [&](const aiTextureType type, common::Texture& dst)
    {
        if (mat->GetTextureCount(type) > 0)
        {
            aiString file;
            mat->GetTexture(type, 0, &file);

            const std::string full = directory_ + file.C_Str();
            dst.Load(full);
        }
    };

    LoadTex(aiTextureType_DIFFUSE,  m.diffuseMap);
    LoadTex(aiTextureType_NORMALS,  m.normalMap);
    LoadTex(aiTextureType_EMISSIVE, m.emissiveMap);
    LoadTex(aiTextureType_SPECULAR, m.metallicMap);
    LoadTex(aiTextureType_SHININESS, m.roughnessMap);
    LoadTex(aiTextureType_AMBIENT, m.aoMap);

    return m;
}
```
Once the materials are loaded, they need to be sent to the shader.
To simplify this, I use the `SetTexture` method from the `Pipeline` abstraction.

This method does two important things: it informs the shader which texture unit to use (via a uniform int)
and it activates that unit to bind the corresponding texture.
```cpp
void Pipeline::SetTexture(std::string_view name, const Texture& texture, int texture_unit) {
  SetInt(name.data(), texture_unit);
  glActiveTexture(GL_TEXTURE0 + texture_unit);
  texture.Bind();
}
```
In the `Bind` method of the `Material` class, I use this abstraction to bind all the maps (BaseColor, Normal, Metallic, etc.) 
in an organized way. I also send booleans so the shader knows if a texture is present or if it should use a default value.
```cpp
void Bind(common::Pipeline& pipeline) const {
    pipeline.SetTexture("diffuseMap",   diffuseMap,   0);
    pipeline.SetTexture("normalMap",    normalMap,    1);
    pipeline.SetTexture("emissiveMap",  emissiveMap,  2);
    pipeline.SetTexture("metallicMap",  metallicMap,  3);
    pipeline.SetTexture("roughnessMap", roughnessMap, 4);
    pipeline.SetTexture("aoMap",        aoMap,        5);


    pipeline.SetBool("useNormalMap", normalMap.get().texture_name != 0);
    pipeline.SetBool("useMetallicMap", metallicMap.get().texture_name != 0);
    // ...
}
```
Then the `common::Texture` class handles the technical side using `stb_image`.

## Normal Mapping

Normal Mapping is used to simulate detail and relief on the car bodywork.
The catch is that the normals in a "Normal Map" are defined in Tangent Space (local to the face).
To use them in World Space for lighting calculations, we need a transition matrix: the TBN Matrix.

I handle this calculation in the Vertex Shader using the `aNormal` and `aTangent` attributes provided by Assimp.
```cpp
  mat3 normalMatrix = mat3(transpose(inverse(finalModel)));
  vec3 T = normalize(normalMatrix * aTangent);
  vec3 N = normalize(normalMatrix * aNormal);
  T = normalize(T - dot(T, N) * N);
  vec3 B = cross(N, T);
```
I use the Gram-Schmidt process here to ensure the tangent remains perfectly orthogonal to the normal.
Then, I calculate the bitangent using a Cross product.This calculation allows me to transition from 
Tangent Space to World Space, which is where I perform all my lighting calculations.

## Back Face Culling

To optimize rendering, I learned about two culling techniques.
The first one is Back-face Culling. It‚Äôs a technique that is very quick to implement; it simply tells the GPU to
skip drawing the back faces of objects. This reduces the number of triangles to process without impacting the final visual result.
```cpp
glEnable(GL_CULL_FACE);
glCullFace(GL_BACK);
glFrontFace(GL_CCW); // D√©finit le sens de lecture des triangles (Counter-Clockwise)
```
In the image below, our camera is positioned inside the car (Wingo).
With Back-Face Culling enabled, all the bodywork faces pointing outward are not rendered. 
That is why we can see right through the model from the inside.

![Back Face Culling example](/assets/back_face_culling.png)

To visually test the concept, we can also switch the mode to Front-Face Culling: in this case, the GPU will 
only display the interior (the back faces) and hide the car's exterior.

## Frustum Culling

The second technique is Frustum Culling. Unlike Back-face culling, which works at the triangle level, 
Frustum Culling operates at the model level. The goal is to avoid sending objects to the GPU if they are outside 
the camera's field of view.

The field of view is represented by a truncated pyramid (the Frustum) defined by six planes: the Near plane, the Far plane,
as well as the top, bottom, left, and right planes.

![Frustum Camera example](/assets/frustum_example.png)

To determine if an object needs to be rendered, I check if it intersects with this viewing volume.
Each model has an AABB (Axis-Aligned Bounding Box) member that is pre-calculated during `Model::Load`.

To put this into practice, I created a very simple `Frustum` class.
It contains the six planes of the view pyramid and features an `IsOnFrustum` method.
This function takes a model's AABB and its `modelMatrix` to determine whether the object is visible or not.
```cpp
class Frustum
{
public:
    void Update(const glm::mat4& viewProjection);
    bool IsOnFrustum(const AABB& box, const glm::mat4& modelMatrix) const;

private:
    std::array<Plane, 6> planes_;
};
```
I integrated this logic directly into my `Camera` abstraction. By combining the projection matrix and the view matrix,
the camera can generate an updated Frustum every frame.
```cpp
Frustum Camera::get_frustum() const
{
    Frustum frustum;
    frustum.Update(projection_matrix_ * get_view_matrix());
    return frustum;
}
```

## GPU Instancing

L'Instancing est une autre technique d'optimisation. C'est ce qui permet
d'afficher des centaines d'objets identiques, comme mes c√¥nes de chantier,
en un seul draw call. Au lieu d'envoyer chaque c√¥ne un par un,
j'envoie un tableau de matrices de transformation au GPU.
![Instancing Cone example](/assets/instancing_example.png)
Pour que cela fonctionne, je dois configurer des attributs de sommets particuliers. 
Une matrice mat4 occupant 4 slots cons√©cutifs (locations 5 √† 8), j'utilise glVertexAttribDivisor.
Cela indique √† OpenGL de ne passer √† la matrice suivante qu'apr√®s avoir dessin√© un objet complet, 
et pas apr√®s chaque sommet.
```cpp
void Mesh::SetupInstancing(const common::VertexBuffer& instance_buffer) {
    vertex_input_.Bind();
    instance_buffer.Bind();

    std::size_t vec4Size = sizeof(glm::vec4);
    glEnableVertexAttribArray(5);
    glVertexAttribPointer(5, 4, GL_FLOAT, GL_FALSE, 4 * vec4Size, (void*)0);
    glEnableVertexAttribArray(6);
    glVertexAttribPointer(6, 4, GL_FLOAT, GL_FALSE, 4 * vec4Size, (void*)(1 * vec4Size));
    glEnableVertexAttribArray(7);
    glVertexAttribPointer(7, 4, GL_FLOAT, GL_FALSE, 4 * vec4Size, (void*)(2 * vec4Size));
    glEnableVertexAttribArray(8);
    glVertexAttribPointer(8, 4, GL_FLOAT, GL_FALSE, 4 * vec4Size, (void*)(3 * vec4Size));

    glVertexAttribDivisor(5, 1);
    glVertexAttribDivisor(6, 1);
    glVertexAttribDivisor(7, 1);
    glVertexAttribDivisor(8, 1);
}
```
C√¥t√© Shader, la logique est tr√®s simple. Si l'instancing est activ√©, j'utilise la matrice 
d'instance re√ßue en attribut (aInstanceMatrix) √† la place de la matrice de mod√®le classique.
Cela permet de positionner chaque c√¥ne √† sa coordonn√©e unique tout en utilisant la m√™me g√©om√©trie.
```cpp
layout (location = 5) in mat4 aInstanceMatrix;
uniform bool useInstancing;

void main() {
    mat4 finalModel = useInstancing ? aInstanceMatrix : model;
    vec4 worldPos = finalModel * vec4(aPos, 1.0);
    
    gl_Position = projection * view * worldPos;
}
```
Enfin, pour le rendu, il suffit d'appeler glDrawElementsInstanced. Mon abstraction Model se charge de 
r√©percuter cet appel sur tous ses sub meshes.

Pour valider que l'instancing fonctionne correctement, j'ai utilis√© RenderDoc afin d'inspecter les appels GPU.
Sur la capture ci-dessous, on peut voir l'appel glDrawElementsInstanced. Au lieu de dessiner 
un seul c√¥ne, le GPU en g√©n√®re 4 en un seul passage en utilisant les donn√©es de mon instance 
buffer (√©videmment on peut en instancier bien plus que 4 üòÖ).
![Instancing RenderDoc example](/assets/instancing_renderdoc.png)
C'est cette technique qui permet de garder un framerate √©lev√© m√™me avec une sc√®ne charg√©e d'objets r√©p√©titifs.

## Cubemap

Avant d'attaquer le gros du moteur, il me fallait un environnement pour placer ma voiture. 
J'ai impl√©ment√© une Skybox. Le principe est simple : on dessine un cube g√©ant autour de la cam√©ra, 
et on applique une texture sur chaque face.

En OpenGL, on utilise une GL_TEXTURE_CUBE_MAP. C'est un type de texture sp√©cial qui g√®re 6 faces
(Right, Left, Top, Bottom, Back, Front) comme une seule entit√©.
![Cubemap example](/assets/cubemap.png)
Pour le chargement dans l'abstraction common::Texture, l'astuce consiste √† boucler sur les fichiers et √† utiliser 
l'enum GL_TEXTURE_CUBE_MAP_POSITIVE_X. Comme les enums des 6 faces se suivent en m√©moire dans OpenGL,
on peut simplement incr√©menter i pour cibler la face suivante.
```cpp
glGenTextures(1, &get().texture_name);
glBindTexture(GL_TEXTURE_CUBE_MAP, get().texture_name);

for (size_t i = 0; i < faces.size(); i++) {
    int width, height, channels;
    unsigned char* data = stbi_load(std::string(faces[i]).c_str(), &width, &height, &channels, 0);

    const GLenum format = (channels == 3) ? GL_RGB : GL_RGBA;
    glTexImage2D(GL_TEXTURE_CUBE_MAP_POSITIVE_X + static_cast<GLenum>(i), 0, format, width, height, 0, format, GL_UNSIGNED_BYTE, data);
    stbi_image_free(data);
}
```
Pour le rendu, il y a deux choses importantes :
- On retire la translation de la matrice de vue.
- On change la fonction de profondeur √† GL_LEQUAL pour √™tre s√ªr que le ciel s'affiche bien au fond.
```cpp
void Skybox::Draw(const glm::mat4& view, const glm::mat4& projection)
{
    glDepthFunc(GL_LEQUAL);
    glDisable(GL_CULL_FACE);

    pipeline_.Bind();
    auto viewNoTranslation = glm::mat4(glm::mat3(view));

    pipeline_.SetMat4("view", glm::value_ptr(viewNoTranslation));
    pipeline_.SetMat4("projection", glm::value_ptr(projection));
    pipeline_.SetTexture("skybox", texture_, 0);
    pipeline_.SetFloat("skyboxIntensity", intensity_);

    cube_mesh_.Draw();

    glDepthFunc(GL_LESS);
}
```
Et maintenant on a une Skybox!
![Skybox Result](/assets/skybox_result.png)

## Deferred Rendering

Le Deferred Rendering est le c≈ìur de mon pipeline de rendu. Contrairement au rendu classique o√π l'on calcule la lumi√®re 
pour chaque objet au moment de le dessiner, le Deferred repose sur une s√©paration claire entre la 
r√©cup√©ration des donn√©es g√©om√©triques et le calcul final de l'image. Cette architecture m'a permis 
d'int√©grer efficacement de nombreuses passes de rendu (Shadows, SSAO, Bloom) sans surcharger le GPU.
![Deferred Overview](/assets/deferred_overview.png)
Le but est simple : arr√™ter de calculer la lumi√®re pour rien. Dans un rendu classique,
on perd beaucoup de performances √† √©clairer des objets qui finissent cach√©s derri√®re d'autres. 
Avec le G-Buffer (Geometry Buffer), on inverse la logique : on attend de savoir exactement quels 
pixels sont visibles √† l'√©cran, et on ne calcule l'√©clairage que pour eux.

## Geometry Buffer

Le G-Buffer est un ensemble de textures dans lesquelles on stocke les informations g√©om√©triques de la sc√®ne lors de la "Geometry Pass".
Au lieu de dessiner une image finale, on remplit plusieurs buffers simultan√©ment :
- Positions
- Normales
- Albedo (BaseColor)
- Emissive

Pour que cela fonctionne techniquement, j'ai cr√©√© une classe GBuffer. L'√©l√©ment le plus important
est l'initialisation des textures via une lambda createTex. Cela me permet de configurer proprement
le MRT (Multiple Render Targets) en attachant chaque buffer (Position, Normale, Albedo, Emissive)
√† un slot sp√©cifique.

```cpp
auto createTex = [&](GLuint& id, const GLint internal, const GLenum type, const int attach) {
    glGenTextures(1, &id);
    glBindTexture(GL_TEXTURE_2D, id);
    glTexImage2D(GL_TEXTURE_2D, 0, internal, width, height, 0, GL_RGBA, type, NULL);
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);
    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0 + attach, GL_TEXTURE_2D, id, 0);
};

createTex(g_position_, GL_RGBA16F, GL_FLOAT, 0);
createTex(g_normal_, GL_RGBA16F, GL_FLOAT, 1);
createTex(g_albedo_, GL_RGBA, GL_UNSIGNED_BYTE, 2);
createTex(g_emissive_, GL_RGBA16F, GL_FLOAT, 3);
```
Une autre m√©thode cruciale est BlitDepthToDefault. Comme le Deferred Rendering s'effectue sur un simple 
Quad √† la fin, on perdrait normalement les informations de profondeur. Cette fonction me permet de 
copier le buffer de profondeur du G-Buffer vers le buffer par d√©faut du syst√®me. 
Cela √©vite que la Skybox ou d'autres objets couvrent les voitures.
```cpp
void GBuffer::BlitDepthToDefault(const unsigned int width, unsigned int const height) const
{
    glBindFramebuffer(GL_READ_FRAMEBUFFER, g_buffer_);
    glBindFramebuffer(GL_DRAW_FRAMEBUFFER, 0);
    glBlitFramebuffer(0, 0, width_, height_, 0, 0, width, height, GL_DEPTH_BUFFER_BIT, GL_NEAREST);
    glBindFramebuffer(GL_FRAMEBUFFER, 0);
}
```
Voici le rendu de mes diff√©rents buffers tel qu'on peut les observer lors d'une capture sur RenderDoc :
- Positions : Chaque pixel stocke ses coordonn√©es mondiales (XYZ).
![GBuffer Position](/assets/gbuffer_position.png)
- Normales : L'orientation de chaque surface.
![GBuffer Normal](/assets/gbuffer_normal.png)
- Albedo : Les couleurs brutes des textures.
![GBuffer Albedo](/assets/gbuffer_albedo.png)
- Emissive : Uniquement les zones qui √©mettent de la lumi√®re.
![GBuffer Emissive](/assets/gbuffer_emissive.png)

## Shadow Pass

Avant d'attaquer le G-Buffer, je dois g√©n√©rer une Shadow Map pour calculer les ombres de ma lumi√®re directionnelle.
Le principe est de faire une passe de rendu du point de vue de la lumi√®re pour stocker
la profondeur de la sc√®ne dans une texture.

Dans ma classe ShadowMap, j'utilise un FBO avec uniquement un GL_DEPTH_ATTACHMENT.
Comme je n'ai pas besoin de couleurs ici, je d√©sactive explicitement les buffers 
de lecture et d'√©criture.
```cpp
glGenFramebuffers(1, &fbo_);
glGenTextures(1, &depth_map_texture_);

glBindTexture(GL_TEXTURE_2D, depth_map_texture_);
glTexImage2D(GL_TEXTURE_2D, 0, GL_DEPTH_COMPONENT24, width_, height_, 0, GL_DEPTH_COMPONENT, GL_UNSIGNED_INT, NULL);

glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);

glBindFramebuffer(GL_FRAMEBUFFER, fbo_);
glFramebufferTexture2D(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_TEXTURE_2D, depth_map_texture_, 0);

constexpr GLenum drawBuffers[] = { GL_NONE };
glDrawBuffers(1, drawBuffers);
glReadBuffer(GL_NONE);

glBindFramebuffer(GL_FRAMEBUFFER, 0);
```
Pour une lumi√®re directionnelle, j'utilise une projection orthographique. 
La matrice lightSpaceMatrix combine cette projection avec une vue positionn√©e face √† la sc√®ne.
```cpp
const glm::mat4 lightProjection = glm::ortho(-orthoSize, orthoSize, -orthoSize, orthoSize, zNear, zFar);
const glm::mat4 lightView = glm::lookAt(lightPos, target, glm::vec3(0.0f, 1.0f, 0.0f));
light_space_matrix_ = lightProjection * lightView;
```
Le shader est minimal. Il transforme les sommets directement dans l'espace de la lumi√®re. Pas besoin de fragment shader complexe,
OpenGL g√®re l'√©criture de la profondeur automatiquement.
```cpp
void main() {
    mat4 finalModel = useInstancing ? aInstanceMatrix : model;
    gl_Position = lightSpaceMatrix * finalModel * vec4(aPos, 1.0);
}
```
Voici √† quoi ressemble cette texture de profondeur captur√©e depuis la source lumineuse
![Shadow Map](/assets/shadow_map.png)
Une fois la texture de profondeur g√©n√©r√©e, je fais le calcul d'ombre dans mon fragment
shader principal, deferred_lit.frag. Pour √©viter le Shadow Acne (des bandes noires dues
√† l'impr√©cision du calcul), j'applique un Bias classique et un Normal Bias qui d√©pend
de l'inclinaison de la face par rapport √† la lumi√®re.
```cpp
float ShadowCalculation(vec4 fragPosLightSpace, vec3 normal, vec3 fragPos) {
    vec3 projCoords = fragPosLightSpace.xyz / fragPosLightSpace.w;
    projCoords = projCoords * 0.5 + 0.5;

    if(projCoords.z > 1.0) return 0.0;

    float currentDepth = projCoords.z;
    vec3 lightDir = normalize(-lights[0].direction);
    float bias = max(shadowBias * (1.0 - dot(normal, lightDir)), shadowBias);

    if(usePCF){
        float shadow = 0.0;
        vec2 texelSize = 1.0 / vec2(textureSize(shadowMap, 0));

        for(int x = -1; x <= 1; ++x) {
            for(int y = -1; y <= 1; ++y) {
                float pcfDepth = texture(shadowMap, projCoords.xy + vec2(x, y) * texelSize).r;
                shadow += currentDepth - bias > pcfDepth  ? 1.0 : 0.0;
            }
        }
        return shadow / 9.0;
    }

    float pcfDepth = texture(shadowMap, projCoords.xy).r;
    return currentDepth - bias > pcfDepth ? 1.0 : 0.0;
}
```
Sur cette image, l'intensit√© de la lumi√®re a √©t√© pouss√©e pour bien voir les ombres. Sans aucun filtrage, 
les bords s'arr√™tent net et l'aspect est tr√®s pixelis√©
![Shadow without PCF](/assets/shadow.png)
Pour corriger √ßa, j'utilise le PCF. Le principe est de faire une "photo" des ombres
autour du pixel actuel en √©chantillonnant une grille 3x3 dans la shadow map,
puis de faire la moyenne. Cela cr√©e un d√©grad√© qui adoucit les contours.
![Shadow with PCF](/assets/shadow_pcf.png)
M√™me avec le PCF, on voit encore parfois une forme de grille dans l'ombre. Pour corriger √ßa, j'utilise un Poisson Disk.
Au lieu de tester les pixels sur une grille carr√©e toute droite, j'utilise 16 points plac√©s de 
fa√ßon plus "al√©atoire" mais bien r√©partis. Je me suis bas√© sur ce [LearnOpenGL](https://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/#poisson-sampling) pour l'impl√©mentation.
```cpp
vec2 poissonDisk[16] = vec2[](
    vec2( -0.94201624, -0.39906216 ),
    vec2( 0.94558609, -0.76890725 ),
    vec2( -0.094184101, -0.92938870 ),
    vec2( 0.34495938, 0.29387760 ),
    vec2( -0.91588581, 0.45771432 ),
    vec2( -0.81544232, -0.87912464 ),
    vec2( -0.38277543, 0.27676845 ),
    vec2( 0.97484398, 0.75648379 ),
    vec2( 0.44323325, -0.97511554 ),
    vec2( 0.53742981, -0.47373420 ),
    vec2( -0.26496911, -0.41893023 ),
    vec2( 0.79197514, 0.19090188 ),
    vec2( -0.24188840, 0.99706507 ),
    vec2( -0.81409955, 0.91437590 ),
    vec2( 0.19984126, 0.78641367 ),
    vec2( 0.14383161, -0.14100790 )
);
```
En multipliant une valeur al√©atoire par 2œÄ (6.28318530718), je g√©n√®re un angle de rotation al√©atoire.
Cet angle sert √† faire pivoter les 16 points du disque diff√©remment pour chaque pixel.
Cela permet de "m√©langer" les samples. La fonction rand vient de cette [discussion Khronos](https://community.khronos.org/t/random-values/75728 ).
```cpp
float shadow = 0.0;
  vec2 texelSize = 1.0 / vec2(textureSize(shadowMap, 0));

  float diskRadius = 2.0;

  float angle = rand(gl_FragCoord.xy) * 6.28318530718;
  float s = sin(angle);
  float c = cos(angle);
  mat2 rotationMatrix = mat2(c, -s, s, c);

  for(int i = 0; i < 16; ++i){
      vec2 offset = rotationMatrix * poissonDisk[i];
      float pcfDepth = texture(shadowMap, projCoords.xy + offset * texelSize * diskRadius).r;
      shadow += currentDepth - bias > pcfDepth ? 1.0 : 0.0;
  }
  return shadow / 16.0;
```
Voici le r√©sultat final. L'ombre est maintenant beaucoup plus "smooth" et naturelle gr√¢ce au
m√©lange du Poisson Disk et de la rotation al√©atoire.
![Shadow with Poisson Sampling + Rotation](/assets/shadow_poisson.png)

## SSAO

Pour ajouter de la profondeur √† la sc√®ne, j'ai appris comment impl√©ment√© le SSAO (Screen Space Ambient Occlusion).
Cette technique simule l'occultation de la lumi√®re ambiante par la g√©om√©trie environnante.
Elle cr√©e des ombres de contact dans les coins, les creux, et surtout sous la voiture,
ce qui permet de bien "ancrer" le mod√®le au sol. Voici un exemple si dessous :
![SSAO example](/assets/ssao_example.png)
Pour simplifier l'impl√©mentation, j'ai cr√©√© une classe SSAO d√©di√©e. Le rendu se fait en deux √©tapes : 
une passe de calcul (qui g√©n√®re l'occlusion brute) et une passe de flou (pour nettoyer le bruit).

Je configure la premi√®re √©tape dans SSAO::Init, qui se charge d'appeler GenerateKernel et GenerateNoiseTexture.
Le calcul repose sur un √©chantillonnage statistique. Plut√¥t que de tirer des rayons partout, je g√©n√®re un Kernel 
(h√©misph√®re) de 64 √©chantillons orient√©s vers l'axe Z.
```cpp
void SSAO::GenerateKernel()
{
    std::uniform_real_distribution<float> random_floats(0.0, 1.0);
    std::default_random_engine generator;

    for (int i = 0; i < 64; ++i)
    {
        glm::vec3 sample(
            random_floats(generator) * 2.0 - 1.0,
            random_floats(generator) * 2.0 - 1.0,
            random_floats(generator)
        );

        sample = glm::normalize(sample);
        sample *= random_floats(generator);

        float scale = static_cast<float>(i) / 64.0;
        scale = std::lerp(0.1f, 1.0f, scale * scale);
        sample *= scale;

        kernel_.push_back(sample);
    }
}
```
Pour la texture de bruit de 4x4 pixels, je la g√©n√®re ici proc√©duralement, mais on pourrait tr√®s
bien utiliser une simple image de bruit charg√©e depuis le disque. La texture est en GL_REPEAT
pour couvrir l'√©cran.
```cpp
std::uniform_real_distribution<float> random_floats(0.0, 1.0);
std::default_random_engine generator;

std::vector<glm::vec4> ssao_noise;

for (unsigned int i = 0; i < 16; i++)
{
    glm::vec4 noise(
        random_floats(generator) * 2.0 - 1.0,
        random_floats(generator) * 2.0 - 1.0,
        0.0f,
        0.0f
    );
    ssao_noise.push_back(noise);
}
```
Une fois le Kernel et le bruit pr√™ts, on passe au rendu.
![SSAO Result 1](/assets/ssao_pass1.png)
Mais souvenez-vous, le travail n'est pas fini. L'image brute est inexploitable √† cause du bruit.
Il nous reste la 2√®me passe : le Flou (Blur).

Dans la fonction de rendu, on voit bien l'encha√Ænement. Apr√®s avoir g√©n√©r√© l'occlusion, 
je binde directement le pipeline de flou (ssao_.Blur()) et je redessine le Quad par-dessus.
```cpp
void SceneSample::RenderSSAOPass(const glm::mat4& view, const glm::mat4& projection)
{
    g_buffer_.BindTextures();
    
    ssao_.BeginGen(projection, view);
    quad_mesh_.Draw();
    ssao_.End();

    ssao_.Blur();
    quad_mesh_.Draw();
    ssao_.End();
}
```
Le shader de flou est tr√®s simple. Contrairement √† un flou Gaussien (qu'on verra plus tard), 
ici une simple moyenne suffit (Box Blur). Je parcours une grille de 4x4 texels 
autour de la position actuelle, j'additionne les valeurs d'occlusion et je divise le tout par 16.
```cpp
vec2 texelSize = 1.0 / vec2(textureSize(ssaoInput, 0));
float result = 0.0;

for (int x = -2; x < 2; ++x)
{
    for (int y = -2; y < 2; ++y)
    {
        vec2 offset = vec2(float(x), float(y)) * texelSize;
        result += texture(ssaoInput, TexCoords + offset).r;
    }
}

FragColor = result / 16.0;
```
Et voil√† le r√©sultat : le bruit a disparu pour laisser place √† une occlusion douce et naturelle!
![SSAO Result 2](/assets/ssao_pass2.png)

## Light PBR

Une fois les donn√©es du G-Buffer et du SSAO pr√™tes, on peut enfin calculer l'√©clairage. L'une des √©tape
√† la fois cool niveau rendu et un peu plus complex a comprendre hehe...

Au d√©part, j'ai impl√©ment√© les 3 types de lumi√®re (Directionnel, Point, Spot) avec le mod√®le Blinn-Phong. C'est l'approche "classique" : 
on additionne une composante Diffuse (la couleur de l'objet) et une composante Sp√©culaire
(le reflet brillant). 
![Halfway example](/assets/halfway.png)
Le Half-way Vector (un vecteur m√©dian entre la direction de la vue et celle
de la lumi√®re) permet d'avoir le reflet sp√©culaire beaucoup plus vite 
qu'un calcul de r√©flexion physique complet.
![Blinn-Phong example](/assets/blinn_phong.png)
C'est deja pas mal, mais pyhsiquement parlant ce n'est pas tr√®s r√©aliste, afin d'avoir des voiture beaucoup
plus cr√©dible je suis pass√© au PBR (Physically Based Rendering) qui est une m√©thode de rendu plus modern.

Pour obtenir ce r√©alisme, le PBR simule la surface des objets au niveau microscopique (Microfacets).
M√™me si une surface parait lisse, au microscope, elle est irr√©guli√®re.
Plus elle est "rugueuse", plus la lumi√®re se disperse de mani√®re chaotique.

Pour piloter cela, on abandonne les vieilles textures "Specular Map" pour
3 de textures physiques :

- Albedo : La couleur de base
- Metallic : Est-ce du m√©tal ou du plastique/bois ? (0.0 ou 1.0).
- Roughness : L'√©tat de surface (Lisse ou Rugueux).

Note d'optimisation : J'ai appris que dans l'industrie, on combine souvent l'Ambient Occlusion,
la Roughness et la Metallic dans une seule texture appel√©e "ARM" (Red, Green, Blue)
pour √©conomiser de la m√©moire.

De mon c√¥t√©, je les ai simplement stock√©es dans les canaux Alpha (.a) du G-Buffer :
```cpp
gNormal.rgb = normalize(Normal);
gNormal.a = useRoughnessMap ? texture(roughnessMap, TexCoords).r : 0.5;
gAlbedo.rgb = texture(diffuseMap, TexCoords).rgb;
gAlbedo.a = useMetallicMap ? texture(metallicMap, TexCoords).r : 0.0;
    ```

Avec le PBR, Tout repose sur l'√©quation de r√©flectance. C'est la formule math√©matique qui somme toute 
la lumi√®re qui arrive pour calculer ce qui renvoie vers l'≈ìil.
![Reflectance Equation](/assets/reflectance_equation.png)
Le terme le plus important dans cette √©quation est le BRDF (fr‚Äã). 
C'est lui qui d√©cide comment la lumi√®re r√©agit au contact de la surface. Dans mon moteur,
j'utilise le standard de l'industrie : le Cook-Torrance BRDF.

Ce mod√®le divise la lumi√®re en deux parties distinctes :
- La Diffuse (Lambert) : La partie "couleur" (r√©fraction interne).
- La Sp√©culaire (Cook-Torrance) : La partie "reflet" (r√©flexion de surface).

L'√©quation compl√®te du BRDF ressemble √† √ßa :
![Cooke-Torrance BRDF](/assets/cook_torrance_brdf.png)
Pour la partie Diffuse, c'est tr√®s simple, c'est juste la couleur divis√©e par PI : c/œÄ.
Pour la partie Sp√©culaire, c'est plus complexe. Elle utilise 3 fonctions physiques 
(Distribution, Fresnel, Geometry) pour simuler les microfacettes.
Je ne vais pas d√©tailler les maths ici, mais pour les curieux, tout est expliqu√© en d√©tail sur
[LearnOpenGL/PBR/Theory](https://learnopengl.com/PBR/Theory)
![Cook-Torrance Specular](/assets/specular.png)
Au final, quand on remplace fr‚Äã dans l'√©quation de base, on obtient la formule compl√®te que mon
shader r√©sout pour chaque lumi√®re :
![Reflectance Equation with Cook-Torrance](/assets/reflectance_equation_cook_torrance.png)
Pour traduire ces maths en code, on retrouve exactement la structure de l'√©quation dans la
boucle de lumi√®re. On calcule les termes D, F, G pour le sp√©culaire, et on respecte la
conservation de l'√©nergie en ajustant la diffuse (kD) en fonction du sp√©culaire (kS).

Voici l'extrait du Fragment Shader qui g√®re une lumi√®re :

```cpp
vec3 L = normalize(lights[i].position - fragPos);
vec3 V = normalize(viewPos - fragPos);
vec3 H = normalize(V + L); // Half-way vector

// (Cook-Torrance)
float NDF = DistributionGGX(N, H, roughness);   // D
float G   = GeometrySmith(N, V, L, roughness);
vec3  F   = fresnelSchlick(max(dot(H, V), 0.0), F0);

// Calcul du Sp√©culaire
vec3 numerator    = NDF * G * F;
float denominator = 4.0 * max(dot(N, V), 0.0) * max(dot(N, L), 0.0);
vec3 specular     = numerator / denominator;

// Conservation d'energie
vec3 kS = F;
vec3 kD = vec3(1.0) - kS;
kD *= 1.0 - metallic; 

// Reflectance Equation
float NdotL = max(dot(N, L), 0.0);
Lo += (kD * albedo / PI + specular) * radiance * NdotL;
```
## Light IBL

Ok, avec √ßa, on a la base du PBR. Mais il manque un truc essentiel pour que √ßa claque vraiment.
Pour l'instant, notre Diffuse se r√©sume juste √† c/œÄ, et le m√©tal ne refl√®te rien du tout.

C'est l√† qu'intervient l'IBL (Image Based Lighting). L'id√©e est d'utiliser la Skybox HDR comme une 
source de lumi√®re g√©ante. Au lieu d'avoir juste une lampe ponctuelle √† une position pr√©cise, 
on consid√®re que chaque pixel du ciel √©met de la lumi√®re.
![IBL example](/assets/ibl_example.png)
Pour r√©soudre l'int√©grale de lumi√®re avec cette infinit√© de sources lumineuses,
on commence par simplifier l'√©quation. On s√©pare le calcul en deux parties distinctes :
la Diffuse et la Sp√©culaire.
![IBL 2 Part formula](/assets/ibl_two_parts.png)
La partie jaune repr√©sente l'int√©grale de la Diffuse, et la partie rose celle du Sp√©culaire. 
En les s√©parant, cela nous permet de r√©soudre d'abord la partie Diffuse (Irradiance) ind√©pendamment.
Vous pouvez voir les d√©tails math√©matiques ici [LearnOpenGL/PBR/IBL/Diffuse-irradiance](https://learnopengl.com/PBR/IBL/Diffuse-irradiance)

Pour faire fonctionner cette √©quation, il nous faut 3 textures sp√©cifiques :
- Irradiance Map : Une version tr√®s floue de la skybox pour la lumi√®re diffuse.
- Prefilter Map : La skybox stock√©e avec diff√©rents niveaux de flou dans les mipmaps
- BRDF LUT (Look Up Table) : C'est la fameuse texture "rouge et verte". Elle contient les valeurs pr√©-calcul√©es de l'√©chelle (Scale) et du d√©calage (Bias) du BRDF.
<div style={{ display: 'flex', justifyContent: 'center', margin: '20px 0' }}>
  <img 
    src="/assets/ibl_brdf_lut.png" 
    alt="IBL BRDF LUT" 
    style={{ maxWidth: '300px', width: '100%', height: 'auto' }} 
  />
</div>

Calculer tout √ßa en temps r√©el serait beaucoup trop lourd. 
J'ai donc g√©n√©r√© ces textures Offline (√† l'avance) en utilisant cmft Studio. 
Je lui donne ma Skybox HDR, et il me sort l'Irradiance et le Prefilter .hdr.
<div style={{ display: 'flex', gap: '10px', justifyContent: 'center', alignItems: 'center' }}>
  <img src="/assets/ibl_skybox.png" alt="IBL Skybox" style={{ maxWidth: '33%', height: 'auto' }} />
  <img src="/assets/ibl_prefilter.png" alt="IBL Prefilter" style={{ maxWidth: '33%', height: 'auto' }} />
  <img src="/assets/ibl_irradiance.png" alt="IBL Irradiance" style={{ maxWidth: '33%', height: 'auto' }} />
</div>
Pour la Diffuse, c'est direct, on tape dans l'Irradiance Map. 
Pour la Sp√©culaire, c'est l√† qu'on applique le Split Sum. On r√©cup√®re la lumi√®re via textureLod
et on mixe le tout avec les valeurs de la LUT.

Une fois qu'on a cet √©clairage ambiant complet, il suffit de l'ajouter au r√©sultat
des lumi√®res directes (Lo) et de l'√©missive.
```cpp
// --- IBL DIFFUSE ---
vec2 uvIrradiance = SampleSphericalMap(N);
vec3 irradiance = texture(irradianceMap, uvIrradiance).rgb * skyboxIntensity;
vec3 diffuse = irradiance * albedo;

// --- IBL SPECULAR ---
const float MAX_REFLECTION_LOD = 8.0;
vec2 uvPrefilter = SampleSphericalMap(R);
vec3 prefilteredColor = textureLod(prefilterMap, uvPrefilter, roughness * MAX_REFLECTION_LOD).rgb;

vec2 brdf  = texture(brdfLUT, vec2(max(dot(N, V), 0.0), roughness)).rg;

vec3 specular = prefilteredColor * (F0 * brdf.x + brdf.y);
specular *= (1.0 - roughness);

vec3 ambient = (kD * diffuse + specular) * ao;
```
Et voil√† le r√©sultat final sur la voiure (Boost)
![IBL Result](/assets/ibl_result.png)

## Framebuffer

Pour rendre possible le Deferred Rendering, le Post-Processing ou m√™me la g√©n√©ration de l'IBL,
je ne peux pas dessiner directement √† l'√©cran (le Backbuffer). J'ai besoin de dessiner "hors-√©cran".
Pour √©viter de r√©√©crire la configuration OpenGL √† chaque fois, j'utilise une abstraction common::Framebuffer. 
Elle g√®re automatiquement :
- La cr√©ation des textures.
- L'attachement au Framebuffer Object (FBO).
- La gestion du Depth/Stencil

Tout le setup se fait dans common::Framebuffer::Load, o√π la
cr√©ation des textures et surtout l'activation du MRT (Multiple Render Targets) est g√©rer.
C'est ce qui permet d'√©crire dans gPosition, gNormal et gAlbedo en m√™me temps.
√áa tient en deux appels :
```cpp
glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0 + i, target, texture_id, 0);
glDrawBuffers(attachment_count, attachments_list.data());
```
Derni√®re √©tape du pipeline : le rendu sur un Screen Quad. Une fois toute la g√©om√©trie calcul√©e, 
au lieu d'afficher l'image directement, je la projette sur un simple quad 
qui recouvre tout l'√©cran. J'ai pu y ajout√© plusieurs filtre diff√©rent (Grayscale, Inverse, Sharpen)
<div style={{ display: 'flex', flexWrap: 'wrap', gap: '15px', justifyContent: 'center' }}> <div style={{ width: '45%', textAlign: 'center' }}> <img src="/assets/fb_normal.png" alt="Normal" style={{ width: '100%', height: 'auto' }} /> <p><strong>Normal</strong></p> </div> <div style={{ width: '45%', textAlign: 'center' }}> <img src="/assets/fb_grayscale.png" alt="Grayscale" style={{ width: '100%', height: 'auto' }} /> <p><strong>Grayscale</strong></p> </div> <div style={{ width: '45%', textAlign: 'center' }}> <img src="/assets/fb_inverse.png" alt="Inverse" style={{ width: '100%', height: 'auto' }} /> <p><strong>Inverse</strong></p> </div> <div style={{ width: '45%', textAlign: 'center' }}> <img src="/assets/fb_sharpen.png" alt="Sharpen" style={{ width: '100%', height: 'auto' }} /> <p><strong>Sharpen</strong></p> </div> </div>

## HDR + Bloom

Par d√©faut, un √©cran standard (LDR) et OpenGL clampent les couleurs entre 0.0 et 1.0.
Le probl√®me, c'est qu'avec le PBR, le soleil ou des reflets m√©talliques peuvent
avoir des intensit√©s de 10.0, 50.0 ou plus. Si on coupe tout √† 1.0, 
on perd toute la nuance des hautes lumi√®res.

En utilisant un Framebuffer avec un format de couleur flottant (GL_RGBA16F). 
Cela permet √† la carte graphique de stocker des valeurs bien sup√©rieures √† 1.0 sans les perdre.
Dans mon abstraction HDRBuffer, j'initialise ce buffer sp√©cial et j'active le MRT 
(Multiple Render Targets) pour extraire la brillance en m√™me temps :
```cpp
glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA16F, width, height, 0, GL_RGBA, GL_FLOAT, NULL);
constexpr unsigned int attachments[2] = { GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1 };
glDrawBuffers(2, attachments);
```
Vu que nos √©crans ne peuvent pas afficher physiquement une valeur de 10.0, 
on doit convertir l'image HDR vers du LDR (0.0 - 1.0) √† la toute fin du pipeline. 
C'est le Tone Mapping. J'utilise l'Exposure Tone Mapping.
Ensuite, il ne faut pas oublier la Gamma Correction. Les calculs physiques se font
en espace lin√©aire, mais on attends (oeil humain et √©crans) une courbe Gamma (puissance 2.2).
```cpp
// post_process.frag
void main()
{
    const float gamma = 2.2;
    vec3 hdrColor = texture(scene, TexCoords).rgb;

    vec3 result = vec3(1.0) - exp(-hdrColor * exposure);

    result = pow(result, vec3(1.0 / gamma));

    FragColor = vec4(result, 1.0);
}
```
Le Bloom, c'est l'effet de halo lumineux autour des objets tr√®s brillants. 
√áa donne l'illusion que la lumi√®re est tellement forte qu'elle "bave" sur la cam√©ra.
![Bloom example](/assets/bloom_example.jpg)
Afin d'avoir du bloom je le fais en 3 √©tapes :

1. Dans le shader PBR principal, pendant qu'on dessine la sc√®ne, 
on s√©pare les pixels qui d√©passent une certaine luminosit√© (Threshold). 
```cpp
// deferred_lit.frag
FragColor = vec4(result, 1.0);

float brightness = dot(result, vec3(0.2126, 0.7152, 0.0722));
BrightColor = brightness > 1.0 ? vec4(result, 1.0) : vec4(0.0, 0.0, 0.0, 1.0);
```
2. Ajouter le [Gaussian Blur](https://en.wikipedia.org/wiki/Gaussian_blur), je prend la texture "BrightColor" et on la floute √©norm√©ment. 
Pour optimiser, au lieu de faire un gros flou en une fois, j'utilise un Ping-Pong Rendering : 
je floute horizontalement, puis verticalement, plusieurs fois de suite. 
C'est g√©r√© par ma classe Bloom qui alterne entre deux Framebuffers :
```cpp
// graphics::Bloom::RenderBloom
bool horizontal = true;
for (int i = 0; i < amount; i++)
{
    glBindFramebuffer(GL_FRAMEBUFFER, pingpongFBO_[horizontal]);
    blur_pipeline_.SetBool("horizontal", horizontal);

    quadMesh.Draw();

    horizontal = !horizontal;
}
```
3. Combinaison Enfin, dans le shader de post-process final, on additionne simplement 
l'image HDR originale avec l'image flout√©e (Bloom). C'est l√† que tout se r√©unit.
```cpp
// post_process.frag
vec3 hdrColor = texture(scene, TexCoords).rgb;
vec3 bloomColor = texture(bloomBlur, TexCoords).rgb;

hdrColor += bloomColor; 
```
Maintenant comparons le r√©sultat sans et avec le bloom!
<div style={{ display: 'flex', gap: '20px', justifyContent: 'center', alignItems: 'flex-start' }}> <div style={{ width: '48%', textAlign: 'center' }}> <img src="/assets/bloom_disable.png" alt="Bloom Disabled" style={{ width: '100%', height: 'auto' }} /> <p><strong>Bloom OFF</strong></p> </div> <div style={{ width: '48%', textAlign: 'center' }}> <img src="/assets/bloom_enable.png" alt="Bloom Enabled" style={{ width: '100%', height: 'auto' }} /> <p><strong>Bloom ON</strong></p> </div> </div>

## Emission

Petit bonus, j'ai ajout√© de l'Emission sur mes mod√®les. 
C'est ce qui permet √† certaines parties de l'objet de "briller" dans le noir
comme des n√©ons pour bien respecter la scene dans cars.

L'int√©gration √©tait franchement simple. C√¥t√© chargement (Assimp), 
il suffisait de r√©cup√©rer la texture correspondante :
```cpp
LoadTex(aiTextureType_EMISSIVE, m.emissiveMap);
```
Ensuite, je la stocke dans mon G-Buffer. 
Je multiplie la couleur par une emissiveStrength pour pouvoir contr√¥ler l'intensit√© du "glow" :
```cpp
// gbuffer.frag
gEmissive = useEmissiveMap ? texture(emissiveMap, TexCoords).rgb * emissiveStrength : vec3(0.0);
```
Et enfin, dans la passe d'√©clairage diff√©r√© (deferred_lit.frag), 
c'est de la lumi√®re pure. Donc je l'ajoute simplement au r√©sultat final, sans calcul d'ombrage. 
C'est l√† que le Bloom fait tout le travail : comme l'intensit√© est √©lev√©e, 
le calcul de brightness va capturer ces pixels et cr√©er le halo lumineux.
```cpp
// deferred_lit.frag
vec3 emissive = texture(gEmissive, TexCoords).rgb;

// Ajout de l'emission
vec3 result = ambient + Lo + emissive;

FragColor = vec4(result, 1.0);

float brightness = dot(result, vec3(0.2126, 0.7152, 0.0722));
BrightColor = brightness > 1.0 ? vec4(result, 1.0) : vec4(0.0, 0.0, 0.0, 1.0);
```
Voici une emissive map que j'ai fais moi meme pour la voiture DJ (La bleu) qui ma
pris 2H30 √† faire (vraiment)
![DJ Emissive](/assets/dj_emissive.png)
Puis le r√©sultat finale dont je suis fi√®re!
![DJ Emissive Result](/assets/dj_emissive_result.png)

# Conclusion

Voil√† ! On a √† peu pr√®s fait le tour des techniques utilis√©es dans ma sc√®ne 3D finale. Durant ce module,
j'ai eu pas mal de moments d'incompr√©hension car je n'ai pas l'habitude d'√™tre aussi bas niveau avec le GPU, 
mais petit √† petit, on s'y fait. J'ai aussi pu voir comment utiliser le GPU pour des calculs autres que le rendu pur (via CUDA).

C'√©tait √† la fois frustrant et gratifiant de voir un r√©sultat qui marche. 
Personnellement, j'ai trouv√© tr√®s int√©ressant de toucher un peu √† OpenGL. 
J'ai appris beaucoup de choses qui m'ont permis de mieux comprendre le fonctionnement des moteurs graphiques,
m√™me si aujourd'hui les gros moteurs utilisent plut√¥t DX11, DX12 ou Vulkan.

L'exp√©rience √©tait cool ! Avoir essay√© de faire une petite sc√®ne sur le th√®me de Cars est ce qui m'a le plus motiv√© sur ce module.
S'il manque quelque chose √† ma sc√®ne, ce serait les ombres pour les Point Lights et pourquoi pas le Cascaded Shadow Mapping.
Mais sinon, je reste tr√®s fier de cette sc√®ne 3D ! Maintenant, quand je jouerai √† n'importe quel AAA,
je remarquerai toujours les techniques que j'ai apprises ici.

N'h√©sitez pas √† checker mon [Github](https://github.com/SAE-Geneve/compgraphsample-Jemow) pour voir l'impl√©mentation plus en d√©tail,

Sur ce, merci pour votre lecture !